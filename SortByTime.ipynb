{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_csv('s1_data.csv')\n",
    "s2 = pd.read_csv('s2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort s1 and s2 by date \n",
    "s1 = s1.sort_values(by=['date'])\n",
    "s2 = s2.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>tile_number</th>\n",
       "      <th>id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>image_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[[[30.033794, -1.947753], [30.033877, -1.99400...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>25</td>\n",
       "      <td>sen12floods_s1_labels_0025_2018_12_15</td>\n",
       "      <td>25</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[[[30.125653, -1.901331], [30.125737, -1.94758...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>24</td>\n",
       "      <td>sen12floods_s1_labels_0024_2018_12_15</td>\n",
       "      <td>24</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>[[[30.079766, -1.947669], [30.079851, -1.99392...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>26</td>\n",
       "      <td>sen12floods_s1_labels_0026_2018_12_15</td>\n",
       "      <td>26</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[[[30.033712, -1.901496], [30.033794, -1.94775...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>22</td>\n",
       "      <td>sen12floods_s1_labels_0022_2018_12_15</td>\n",
       "      <td>22</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[[[30.079683, -1.901415], [30.079766, -1.94766...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>23</td>\n",
       "      <td>sen12floods_s1_labels_0023_2018_12_15</td>\n",
       "      <td>23</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              geometry  label        date  \\\n",
       "131  [[[30.033794, -1.947753], [30.033877, -1.99400...      0  2018-12-15   \n",
       "121  [[[30.125653, -1.901331], [30.125737, -1.94758...      0  2018-12-15   \n",
       "141  [[[30.079766, -1.947669], [30.079851, -1.99392...      0  2018-12-15   \n",
       "101  [[[30.033712, -1.901496], [30.033794, -1.94775...      0  2018-12-15   \n",
       "111  [[[30.079683, -1.901415], [30.079766, -1.94766...      0  2018-12-15   \n",
       "\n",
       "     tile_number                                     id  location_id  \\\n",
       "131           25  sen12floods_s1_labels_0025_2018_12_15           25   \n",
       "121           24  sen12floods_s1_labels_0024_2018_12_15           24   \n",
       "141           26  sen12floods_s1_labels_0026_2018_12_15           26   \n",
       "101           22  sen12floods_s1_labels_0022_2018_12_15           22   \n",
       "111           23  sen12floods_s1_labels_0023_2018_12_15           23   \n",
       "\n",
       "                                             image_dir  \n",
       "131  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "121  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "141  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "101  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "111  sen12flood/sen12floods_s1_source/sen12floods_s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir TimewiseCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for same date create csv files\n",
    "base_path= \"TimewiseCSV\\\\\"\n",
    "k={}\n",
    "for i in range(len(s1)):\n",
    "    date = s1['date'][i]\n",
    "    k[date]=0\n",
    "    k[date]+=1\n",
    "    s1_date = s1[s1['date']==date]\n",
    "    s2_date = s2[s2['date']==date]\n",
    "    if(k[date]==1):\n",
    "        s1_date.to_csv(base_path+str(date)+'_s1.csv',index=False)\n",
    "        s2_date.to_csv(base_path+str(date)+'_s2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unbatching a tensor is only supported for rank >= 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18400\\1354125805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[1;34m\"sen12flood\\\\sen12floods_s1_source\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[0mmodel_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m     \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid `element`. `element` should not be empty.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     self._structure = nest.map_structure(\n\u001b[0m\u001b[0;32m     39\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n\u001b[0;32m     40\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 226\u001b[1;33m       structure[0], [func(*x) for x in entries])\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 226\u001b[1;33m       structure[0], [func(*x) for x in entries])\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(component_spec)\u001b[0m\n\u001b[0;32m     37\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid `element`. `element` should not be empty.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     self._structure = nest.map_structure(\n\u001b[1;32m---> 39\u001b[1;33m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adrij\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_spec.py\u001b[0m in \u001b[0;36m_unbatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    364\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_unbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unbatching a tensor is only supported for rank >= 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unbatching a tensor is only supported for rank >= 1"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    This class provides a set of parameters and constants that may be used throughout a machine learning \n",
    "    pipeline for image classification, specifically in the context of identifying flooded areas.\n",
    "    \"\"\"\n",
    "    seed = 3 # random initialization of weights in a machine learning model\n",
    "    img_size = (256,256) # representing the dimensions of an image, specifically 256 x 256 pixels.\n",
    "    BATCH_SIZE = 3 #  representing the number of samples that will be fed to a machine learning model during training.\n",
    "    Autotune = tf.data.AUTOTUNE # a constant value from the tf.data.AUTOTUNE module that enables dynamic \n",
    "    # allocation of computational resources to improve performance.\n",
    "    validation_size = 0.2 # a float value of 0.2 representing the fraction of the training dataset to be used for validation during training.\n",
    "    class_dict= {0:'No Flooding', \n",
    "                 1: 'Flooding'}\n",
    "    \n",
    "    test_run = False # in training mode\n",
    "\n",
    "def get_tf_dataset(image_paths,\n",
    "                   labels=None, # put none for test data set\n",
    "                   image_processing_fn=None,\n",
    "                   augment_fn = None\n",
    "                  ):\n",
    "    \n",
    "    \n",
    "    '''returns a tf dataset object\n",
    "    Inputs: \n",
    "    image_paths : paths to images\n",
    "    labels: labels of each image\n",
    "    image_processing_fn:  function to load and preprocess images \n",
    "    augment_fn : function to augment images '''\n",
    "    \n",
    "    #seperate datasets\n",
    "    if labels is not None:\n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    #load images \n",
    "    image_dataset = image_dataset.map(image_processing_fn,\n",
    "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    if augment_fn is not None:\n",
    "        \n",
    "        image_dataset = image_dataset.map(augment_fn,\n",
    "                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    \n",
    "    if labels is not None:\n",
    "        return tf.data.Dataset.zip((image_dataset,labels_dataset))\n",
    "    \n",
    "    \n",
    "    return image_dataset\n",
    "\n",
    "\n",
    "\n",
    "def optimize_pipeline(tf_dataset,\n",
    "                      batch_size = CFG.BATCH_SIZE,\n",
    "                      Autotune_fn = CFG.Autotune,\n",
    "                      cache= False,\n",
    "                      batch = True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # prefetch(load the data with cpu,while gpu is training) the data in memory \n",
    "    tf_dataset = tf_dataset.prefetch(buffer_size=Autotune_fn)  \n",
    "    if cache:\n",
    "        tf_dataset = tf_dataset.cache()                        # store data in RAM  \n",
    "        \n",
    "    tf_dataset =  tf_dataset.shuffle(buffer_size=50)         # shuffle \n",
    "    \n",
    "    if batch:\n",
    "        tf_dataset = tf_dataset.batch(batch_size)              #split the data in batches  \n",
    "    \n",
    "    return tf_dataset\n",
    "\n",
    "# Sentinel 1 dataset (not using augmentation here)\n",
    "\n",
    "\n",
    "def model_prediction(image):\n",
    "    \n",
    "    SAR_CNN = tf.keras.models.load_model('CNN_models/SAR_CNN.h5',\n",
    "                                     custom_objects={'f1_score':f1_score,\n",
    "                                                     'recall_m':recall_m,\n",
    "                                                     'precision_m':precision_m\n",
    "                                                     }\n",
    "                                    )\n",
    "\n",
    "    pred = np.argmax(SAR_CNN.predict(image[tf.newaxis,:,:,:]))\n",
    "    prd = int(pred.ravel())\n",
    "    return pred\n",
    "\n",
    "image_paths=  \"sen12flood\\sen12floods_s1_source\\sen12floods_s1_source\\sen12floods_s1_source_0_2019_02_18\"  \n",
    "img=tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "model_prediction(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
