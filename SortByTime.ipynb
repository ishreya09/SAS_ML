{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_csv('s1_data.csv')\n",
    "s2 = pd.read_csv('s2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort s1 and s2 by date \n",
    "s1 = s1.sort_values(by=['date'])\n",
    "s2 = s2.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>tile_number</th>\n",
       "      <th>id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>image_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[[[30.033794, -1.947753], [30.033877, -1.99400...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>25</td>\n",
       "      <td>sen12floods_s1_labels_0025_2018_12_15</td>\n",
       "      <td>25</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[[[30.125653, -1.901331], [30.125737, -1.94758...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>24</td>\n",
       "      <td>sen12floods_s1_labels_0024_2018_12_15</td>\n",
       "      <td>24</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>[[[30.079766, -1.947669], [30.079851, -1.99392...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>26</td>\n",
       "      <td>sen12floods_s1_labels_0026_2018_12_15</td>\n",
       "      <td>26</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[[[30.033712, -1.901496], [30.033794, -1.94775...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>22</td>\n",
       "      <td>sen12floods_s1_labels_0022_2018_12_15</td>\n",
       "      <td>22</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[[[30.079683, -1.901415], [30.079766, -1.94766...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>23</td>\n",
       "      <td>sen12floods_s1_labels_0023_2018_12_15</td>\n",
       "      <td>23</td>\n",
       "      <td>sen12flood/sen12floods_s1_source/sen12floods_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              geometry  label        date  \\\n",
       "131  [[[30.033794, -1.947753], [30.033877, -1.99400...      0  2018-12-15   \n",
       "121  [[[30.125653, -1.901331], [30.125737, -1.94758...      0  2018-12-15   \n",
       "141  [[[30.079766, -1.947669], [30.079851, -1.99392...      0  2018-12-15   \n",
       "101  [[[30.033712, -1.901496], [30.033794, -1.94775...      0  2018-12-15   \n",
       "111  [[[30.079683, -1.901415], [30.079766, -1.94766...      0  2018-12-15   \n",
       "\n",
       "     tile_number                                     id  location_id  \\\n",
       "131           25  sen12floods_s1_labels_0025_2018_12_15           25   \n",
       "121           24  sen12floods_s1_labels_0024_2018_12_15           24   \n",
       "141           26  sen12floods_s1_labels_0026_2018_12_15           26   \n",
       "101           22  sen12floods_s1_labels_0022_2018_12_15           22   \n",
       "111           23  sen12floods_s1_labels_0023_2018_12_15           23   \n",
       "\n",
       "                                             image_dir  \n",
       "131  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "121  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "141  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "101  sen12flood/sen12floods_s1_source/sen12floods_s...  \n",
       "111  sen12flood/sen12floods_s1_source/sen12floods_s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir TimewiseCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for same date create csv files\n",
    "base_path= \"TimewiseCSV\\\\\"\n",
    "k={}\n",
    "for i in range(len(s1)):\n",
    "    date = s1['date'][i]\n",
    "    k[date]=0\n",
    "    k[date]+=1\n",
    "    s1_date = s1[s1['date']==date]\n",
    "    s2_date = s2[s2['date']==date]\n",
    "    if(k[date]==1):\n",
    "        s1_date.to_csv(base_path+str(date)+'_s1.csv',index=False)\n",
    "        s2_date.to_csv(base_path+str(date)+'_s2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 861ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import os\n",
    "import rasterio as rio\n",
    "import cv2\n",
    "\n",
    "def load_raster(filepath): # filepath of the raster file to be loaded\n",
    "    '''load a single band raster'''\n",
    "    with rio.open(filepath) as file: \n",
    "        # the squeeze method is called on the resulting array to remove any singleton dimensions \n",
    "        # (i.e., dimensions with size 1). This is done using the axis=0 argument, \n",
    "        # which tells squeeze to remove any singleton dimensions along the first axis.  \n",
    "        raster = file.read().squeeze(axis=0)\n",
    "\n",
    "        \n",
    "    #we aregetting back the 2D image from singleton(1D)\n",
    "    return raster\n",
    "\n",
    "def load_s1_tiffs(folder,\n",
    "                  scaling_values=[50.,100.]):\n",
    "    images = []\n",
    "    i = 0\n",
    "    for im in sorted(os.listdir(folder)):\n",
    "         \n",
    "        if im.rsplit('.',maxsplit=1)[1] == 'tif':\n",
    "            \n",
    "            path = folder + '/' + im\n",
    "            band = load_raster(path)\n",
    "            band = band / scaling_values[i]\n",
    "            \n",
    "            band = cv2.resize(band,\n",
    "                              CFG.img_size)\n",
    "            \n",
    "            images.append(band)\n",
    "            i+=1 \n",
    "                    \n",
    "    return np.dstack(images)\n",
    "\n",
    "\n",
    "def load_s2_tiffs(folder,\n",
    "                  scaling_value=10000.):\n",
    "    images = []\n",
    "    for im in sorted(os.listdir(folder)):\n",
    "        if im.rsplit('.',maxsplit=1)[1] == 'tif':    \n",
    "            path = folder + '/' + im\n",
    "            band = load_raster(path)\n",
    "            band = band/ scaling_value\n",
    "            \n",
    "            band = cv2.resize(band,CFG.img_size)\n",
    "            images.append(band)   \n",
    "\n",
    "    return np.dstack(images)\n",
    "                    \n",
    "    \n",
    "def tf_load_s1(path):    \n",
    "    path = path.numpy().decode('utf-8')\n",
    "    return load_s1_tiffs(path)\n",
    "    \n",
    "    \n",
    "\n",
    "def tf_load_s2(path):    \n",
    "    path = path.numpy().decode('utf-8')\n",
    "    return load_s2_tiffs(path)\n",
    "\n",
    "    \n",
    "def process_image_s1(filename):\n",
    "    '''function for preprocessing in tensorflow data'''\n",
    "    \n",
    "    return tf.py_function(tf_load_s1, \n",
    "                          [filename], \n",
    "                          tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "def process_image_s2(filename):\n",
    "    '''function for preprocessing in tensorflow data'''\n",
    "    \n",
    "    return tf.py_function(tf_load_s2, \n",
    "                          [filename], \n",
    "                          tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    This class provides a set of parameters and constants that may be used throughout a machine learning \n",
    "    pipeline for image classification, specifically in the context of identifying flooded areas.\n",
    "    \"\"\"\n",
    "    seed = 3 # random initialization of weights in a machine learning model\n",
    "    img_size = (256,256) # representing the dimensions of an image, specifically 256 x 256 pixels.\n",
    "    BATCH_SIZE = 3 #  representing the number of samples that will be fed to a machine learning model during training.\n",
    "    Autotune = tf.data.AUTOTUNE # a constant value from the tf.data.AUTOTUNE module that enables dynamic \n",
    "    # allocation of computational resources to improve performance.\n",
    "    validation_size = 0.2 # a float value of 0.2 representing the fraction of the training dataset to be used for validation during training.\n",
    "    class_dict= {0:'No Flooding', \n",
    "                 1: 'Flooding'}\n",
    "    \n",
    "    test_run = False # in training mode\n",
    "\n",
    "def get_tf_dataset(image_paths,\n",
    "                   labels=None, # put none for test data set\n",
    "                   image_processing_fn=None,\n",
    "                   augment_fn = None\n",
    "                  ):\n",
    "    \n",
    "    \n",
    "    '''returns a tf dataset object\n",
    "    Inputs: \n",
    "    image_paths : paths to images\n",
    "    labels: labels of each image\n",
    "    image_processing_fn:  function to load and preprocess images \n",
    "    augment_fn : function to augment images '''\n",
    "    \n",
    "    #seperate datasets\n",
    "    if labels is not None:\n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    #load images \n",
    "    image_dataset = image_dataset.map(image_processing_fn,\n",
    "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    if augment_fn is not None:\n",
    "        \n",
    "        image_dataset = image_dataset.map(augment_fn,\n",
    "                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    \n",
    "    if labels is not None:\n",
    "        return tf.data.Dataset.zip((image_dataset,labels_dataset))\n",
    "    \n",
    "    \n",
    "    return image_dataset\n",
    "\n",
    "\n",
    "\n",
    "def optimize_pipeline(tf_dataset,\n",
    "                      batch_size = CFG.BATCH_SIZE,\n",
    "                      Autotune_fn = CFG.Autotune,\n",
    "                      cache= False,\n",
    "                      batch = True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # prefetch(load the data with cpu,while gpu is training) the data in memory \n",
    "    tf_dataset = tf_dataset.prefetch(buffer_size=Autotune_fn)  \n",
    "    if cache:\n",
    "        tf_dataset = tf_dataset.cache()                        # store data in RAM  \n",
    "        \n",
    "    tf_dataset =  tf_dataset.shuffle(buffer_size=50)         # shuffle \n",
    "    \n",
    "    if batch:\n",
    "        tf_dataset = tf_dataset.batch(batch_size)              #split the data in batches  \n",
    "    \n",
    "    return tf_dataset\n",
    "\n",
    "# Sentinel 1 dataset (not using augmentation here)\n",
    "\n",
    "\n",
    "SAR_CNN = tf.keras.models.load_model('CNN_models/SAR_CNN.h5',\n",
    "                                     custom_objects={'f1_score': f1_score,\n",
    "                                                     'precision_m': precision_m,\n",
    "                                                     'recall_m': recall_m})\n",
    "\n",
    "def model_prediction(image):\n",
    "    \n",
    "\n",
    "    pred = np.argmax(SAR_CNN.predict(image[tf.newaxis,:,:,:]))\n",
    "    # prd = int(pred.ravel())\n",
    "    return pred\n",
    "\n",
    "def set_data(s_data):\n",
    "    S1_dataset = optimize_pipeline(tf_dataset=get_tf_dataset(image_paths = s_data.image_dir.values,\n",
    "                                                labels = s_data.label,\n",
    "                                                image_processing_fn = process_image_s1),\n",
    "                                    \n",
    "                                    batch_size = 3 * CFG.BATCH_SIZE)\n",
    "    return S1_dataset\n",
    "\n",
    "data= set_data(pd.read_csv('TimewiseCSV\\\\2018-12-16_s1.csv'))\n",
    "for images,labels in data:\n",
    "    # print(images.shape)\n",
    "    # print(labels.shape)\n",
    "    # print(labels)\n",
    "    # print(\"IMG\",images)\n",
    "    for k in range(len(images)):\n",
    "        # print(\"img\",images[k])\n",
    "        try:\n",
    "            print(model_prediction(images[k]))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'rasterio' has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18400\\813844911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Apply a speckle filter using median filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mvv_img_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvv_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mvh_img_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvh_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'rasterio' has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load VV and VH polarization images\n",
    "with rio.open('sen12flood\\\\sen12floods_s1_source\\\\sen12floods_s1_source\\\\sen12floods_s1_source_0286_2019_03_20\\\\VV.tif') as vv_ds:\n",
    "    vv_img = vv_ds.read(1)\n",
    "with rio.open('sen12flood\\\\sen12floods_s1_source\\\\sen12floods_s1_source\\\\sen12floods_s1_source_0286_2019_03_20\\\\VV.tif') as vh_ds:\n",
    "    vh_img = vh_ds.read(1)\n",
    "\n",
    "# Convert the images to float32 data type\n",
    "vv_img = vv_img.astype(np.float32)\n",
    "vh_img = vh_img.astype(np.float32)\n",
    "\n",
    "# Apply a speckle filter using median filter\n",
    "vv_img_filtered = rio.features.median_filter(vv_img, size=5)\n",
    "vh_img_filtered = rio.features.median_filter(vh_img, size=5)\n",
    "\n",
    "# Calculate the backscatter coefficient for VV and VH polarization\n",
    "calibration_factor = 1.0\n",
    "range_distance = 1.0\n",
    "sigma_vv = 10.0 * np.log10(np.square(vv_img_filtered) / (calibration_factor * np.power(range_distance, 4)))\n",
    "sigma_vh = 10.0 * np.log10(np.square(vh_img_filtered) / (calibration_factor * np.power(range_distance, 4)))\n",
    "\n",
    "# Apply radiometric calibration\n",
    "sigma_vv = sigma_vv - np.mean(sigma_vv)\n",
    "sigma_vh = sigma_vh - np.mean(sigma_vh)\n",
    "\n",
    "# Calculate the green, red, and near-infrared bands\n",
    "green_band = sigma_vv / sigma_vh\n",
    "red_band = sigma_vh / sigma_vv\n",
    "nir_band = (sigma_vh - sigma_vv) / (sigma_vh + sigma_vv)\n",
    "\n",
    "# Normalize the bands to 0-255 range\n",
    "green_band_norm = np.uint8(255 * (green_band - green_band.min()) / (green_band.max() - green_band.min()))\n",
    "red_band_norm = np.uint8(255 * (red_band - red_band.min()) / (red_band.max() - red_band.min()))\n",
    "nir_band_norm = np.uint8(255 * (nir_band - nir_band.min()) / (nir_band.max() - nir_band.min()))\n",
    "\n",
    "# Save the bands as separate images\n",
    "with rasterio.open('green_band.tif', 'w', driver='GTiff', width=vv_img.shape[1], height=vv_img.shape[2], count=1, dtype=green_band_norm.dtype) as green_ds:\n",
    "    green_ds.write(green_band_norm, 1)\n",
    "with rasterio.open('red_band.tif', 'w', driver='GTiff', width=vv_img.shape[1], height=vv_img.shape[2], count=1, dtype=red_band_norm.dtype) as red_ds:\n",
    "    red_ds.write(red_band_norm, 1)\n",
    "with rasterio.open('nir_band.tif', 'w', driver='GTiff', width=vv_img.shape[1], height=vv_img.shape[2], count=1, dtype=nir_band_norm.dtype) as nir_ds:\n",
    "    nir_ds.write(nir_band_norm, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
